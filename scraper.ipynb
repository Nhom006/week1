{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scraper.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mTMmd7L3hJE"
      },
      "source": [
        "!pip install scrapy"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_63ULdH3uTR"
      },
      "source": [
        "import scrapy \n",
        "import pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4yIX5zk3-So",
        "outputId": "60c746b7-947f-47be-e21b-8c499d40462c"
      },
      "source": [
        "%cd \"/content/drive/MyDrive/Colab Notebooks/crawler/\"\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/crawler\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhCHD7-WaZdw",
        "outputId": "7fe2e853-b7f0-40b3-933c-8bc4ff84512d"
      },
      "source": [
        "!bash"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bash: cannot set terminal process group (61): Inappropriate ioctl for device\n",
            "bash: no job control in this shell\n",
            ".co.uk/topic/coronavirus/'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/scrapy\", line 8, in <module>\n",
            "    sys.exit(execute())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/cmdline.py\", line 144, in execute\n",
            "    cmd.crawler_process = CrawlerProcess(settings)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/crawler.py\", line 280, in __init__\n",
            "    super().__init__(settings)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/crawler.py\", line 152, in __init__\n",
            "    self.spider_loader = self._get_spider_loader(settings)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/crawler.py\", line 146, in _get_spider_loader\n",
            "    return loader_cls.from_settings(settings.frozencopy())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/spiderloader.py\", line 67, in from_settings\n",
            "    return cls(settings)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/spiderloader.py\", line 24, in __init__\n",
            "    self._load_all_spiders()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/spiderloader.py\", line 51, in _load_all_spiders\n",
            "    for module in walk_modules(name):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/utils/misc.py\", line 88, in walk_modules\n",
            "    submod = import_module(fullpath)\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/crawler/crawler/spiders/spider.py\", line 4, in <module>\n",
            "    class QuotesSpider(scrapy.Spider):\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/crawler/crawler/spiders/spider.py\", line 18, in QuotesSpider\n",
            "    next_page = response.css('')       \n",
            "NameError: name 'response' is not defined\n",
            ".co.uk/topic/coronavirus/'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/scrapy\", line 8, in <module>\n",
            "    sys.exit(execute())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/cmdline.py\", line 144, in execute\n",
            "    cmd.crawler_process = CrawlerProcess(settings)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/crawler.py\", line 280, in __init__\n",
            "    super().__init__(settings)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/crawler.py\", line 152, in __init__\n",
            "    self.spider_loader = self._get_spider_loader(settings)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/crawler.py\", line 146, in _get_spider_loader\n",
            "    return loader_cls.from_settings(settings.frozencopy())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/spiderloader.py\", line 67, in from_settings\n",
            "    return cls(settings)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/spiderloader.py\", line 24, in __init__\n",
            "    self._load_all_spiders()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/spiderloader.py\", line 51, in _load_all_spiders\n",
            "    for module in walk_modules(name):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scrapy/utils/misc.py\", line 88, in walk_modules\n",
            "    submod = import_module(fullpath)\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/content/drive/My Drive/Colab Notebooks/crawler/crawler/spiders/spider.py\", line 4, in <module>\n",
            "    class QuotesSpider(scrapy.Spider):\n",
            "  File \"/content/drive/My Drive/Colab Notebooks/crawler/crawler/spiders/spider.py\", line 18, in QuotesSpider\n",
            "    next_page = response.css('')       \n",
            "NameError: name 'response' is not defined\n",
            ".co.uk/topic/coronavirus/'\n",
            "2021-06-12 14:55:49 [scrapy.utils.log] INFO: Scrapy 2.5.0 started (bot: crawler)\n",
            "2021-06-12 14:55:49 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.2.0, Python 3.7.10 (default, May  3 2021, 02:48:31) - [GCC 7.5.0], pyOpenSSL 20.0.1 (OpenSSL 1.1.1k  25 Mar 2021), cryptography 3.4.7, Platform Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2021-06-12 14:55:49 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2021-06-12 14:55:49 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'crawler',\n",
            " 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n",
            " 'LOGSTATS_INTERVAL': 0,\n",
            " 'NEWSPIDER_MODULE': 'crawler.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['crawler.spiders']}\n",
            "2021-06-12 14:55:49 [scrapy.extensions.telnet] INFO: Telnet Password: 73d022c5f02dfd20\n",
            "2021-06-12 14:55:49 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage']\n",
            "2021-06-12 14:55:49 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2021-06-12 14:55:49 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2021-06-12 14:55:49 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2021-06-12 14:55:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2021-06-12 14:55:49 [scrapy.core.engine] INFO: Spider opened\n",
            "2021-06-12 14:55:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.thesun.co.uk/robots.txt> (referer: None)\n",
            "2021-06-12 14:55:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.thesun.co.uk/topic/coronavirus/> (referer: None)\n",
            "[s] Available Scrapy objects:\n",
            "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
            "[s]   crawler    <scrapy.crawler.Crawler object at 0x7f3a62780bd0>\n",
            "[s]   item       {}\n",
            "[s]   request    <GET https://www.thesun.co.uk/topic/coronavirus/>\n",
            "[s]   response   <200 https://www.thesun.co.uk/topic/coronavirus/>\n",
            "[s]   settings   <scrapy.settings.Settings object at 0x7f3a626db810>\n",
            "[s]   spider     <DefaultSpider 'default' at 0x7f3a621701d0>\n",
            "[s] Useful shortcuts:\n",
            "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
            "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n",
            "[s]   shelp()           Shell help (print this help)\n",
            "[s]   view(response)    View response in a browser\n",
            "\u001b[?1l\u001b[6n\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;38;5;10;1m1\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[8D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;38;5;10;1m1\u001b[0;38;5;28m]: \u001b[0mresponse\u001b[0m.\u001b[0mcss\u001b[0m(\u001b[0;38;5;130m'a.pagination-next'\u001b[0m)\u001b[0m.\u001b[0mattrib\u001b[0m[\u001b[0;38;5;130m'href'\u001b[0m]\u001b[56D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;38;5;9;1m1\u001b[0;38;5;88m]: \u001b[0m'https://www.thesun.co.uk/topic/coronavirus/page/2/'\n",
            "\n",
            "\u001b[?1l\u001b[6n\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;38;5;10;1m2\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[8D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;38;5;10;1m2\u001b[0;38;5;28m]: \u001b[0mquit\u001b[0m()\u001b[14D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\n",
            ".co.uk/topic/coronavirus/'\n",
            "2021-06-12 15:02:21 [scrapy.utils.log] INFO: Scrapy 2.5.0 started (bot: crawler)\n",
            "2021-06-12 15:02:21 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.2.0, Python 3.7.10 (default, May  3 2021, 02:48:31) - [GCC 7.5.0], pyOpenSSL 20.0.1 (OpenSSL 1.1.1k  25 Mar 2021), cryptography 3.4.7, Platform Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2021-06-12 15:02:21 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2021-06-12 15:02:21 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'crawler',\n",
            " 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n",
            " 'LOGSTATS_INTERVAL': 0,\n",
            " 'NEWSPIDER_MODULE': 'crawler.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['crawler.spiders']}\n",
            "2021-06-12 15:02:21 [scrapy.extensions.telnet] INFO: Telnet Password: 31a80e080b81fed5\n",
            "2021-06-12 15:02:21 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage']\n",
            "2021-06-12 15:02:21 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2021-06-12 15:02:21 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2021-06-12 15:02:21 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2021-06-12 15:02:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2021-06-12 15:02:21 [scrapy.core.engine] INFO: Spider opened\n",
            "2021-06-12 15:02:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.thesun.co.uk/robots.txt> (referer: None)\n",
            "2021-06-12 15:02:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.thesun.co.uk/topic/coronavirus/> (referer: None)\n",
            "[s] Available Scrapy objects:\n",
            "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
            "[s]   crawler    <scrapy.crawler.Crawler object at 0x7f5b17032b90>\n",
            "[s]   item       {}\n",
            "[s]   request    <GET https://www.thesun.co.uk/topic/coronavirus/>\n",
            "[s]   response   <200 https://www.thesun.co.uk/topic/coronavirus/>\n",
            "[s]   settings   <scrapy.settings.Settings object at 0x7f5b16f90e90>\n",
            "[s]   spider     <DefaultSpider 'default' at 0x7f5b16a292d0>\n",
            "[s] Useful shortcuts:\n",
            "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
            "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n",
            "[s]   shelp()           Shell help (print this help)\n",
            "[s]   view(response)    View response in a browser\n",
            "\u001b[?1l\u001b[6n\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;38;5;10;1m1\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[8D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;38;5;10;1m1\u001b[0;38;5;28m]: \u001b[0mresponse\u001b[0m.\u001b[0mcss\u001b[0m(\u001b[0;38;5;130m'a.text-anchor-wrap p::text'\u001b[0m)\u001b[0m.\u001b[0mget\u001b[0m()\u001b[56D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;38;5;9;1m1\u001b[0;38;5;88m]: \u001b[0m'\\n\\t\\t\\tTwo MONKEYPOX cases and outbreak of drug-resistant tuberculosis found in UK\\t\\t'\n",
            "\n",
            "\u001b[?1l\u001b[6n\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;38;5;10;1m2\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[8D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;38;5;10;1m2\u001b[0;38;5;28m]: \u001b[0mresponse\u001b[0m.\u001b[0mcss\u001b[0m(\u001b[0;38;5;130m'a.text-anchor-wrap::attr(href)'\u001b[0m)\u001b[0m.\u001b[0mget\u001b[0m()\u001b[60D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;38;5;9;1m2\u001b[0;38;5;88m]: \u001b[0m'https://www.thesun.co.uk/news/15128481/covid-vaccine-test-uk-cases-monkeypox-tuberculosis-latest-news/'\n",
            "\n",
            "\u001b[?1l\u001b[6n\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;38;5;10;1m3\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[8D\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;38;5;10;1m3\u001b[0;38;5;28m]: \u001b[0mresponse\u001b[0m.\u001b[0mcss\u001b[0m(\u001b[0;38;5;130m'a.text-anchor-wrap p::text'\u001b[0m)\u001b[0m.\u001b[0mget\u001b[0m()\u001b[56D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004l\u001b[0m\u001b[?7h\u001b[0;38;5;88mOut[\u001b[0;38;5;9;1m3\u001b[0;38;5;88m]: \u001b[0m'\\n\\t\\t\\tTwo MONKEYPOX cases and outbreak of drug-resistant tuberculosis found in UK\\t\\t'\n",
            "\n",
            "\u001b[?1l\u001b[6n\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mIn [\u001b[0;38;5;10;1m4\u001b[0;38;5;28m]: \u001b[8D\u001b[8C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN8WdF2iULFz"
      },
      "source": [
        "response.css('author::text').getall()[0]\n",
        "scrapy shell 'https://www.thesun.co.uk/topic/coronavirus/'\n",
        "response.css('title')[1].getall()\n",
        "response.css('a.pagination-next').attrib['href']\n",
        "scrapy crawl posts -o time.json\n",
        "response.css('div.col-sm-6 a::attr(href)').get()\n",
        "response.css('a.text-anchor-wrap p::text').get().replaceAll(\"\\\\s+\", \"\")\n",
        "scrapy crawl posts \n",
        "response.css('.card__text a').get()\n",
        "response.css('article.partial.tile.media.image-top.type-article').css('h3 a::attr(href)').get()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ELbMWZouBNN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}